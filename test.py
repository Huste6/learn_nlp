import nltk
import spacy
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# T·∫£i d·ªØ li·ªáu c·∫ßn thi·∫øt
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt', force=True)

# T·∫£i m√¥ h√¨nh ti·∫øng Anh c·ªßa spaCy
nlp = spacy.load("en_core_web_sm")

# VƒÉn b·∫£n m·∫´u
text = "Natural Language Processing (NLP) is a sub-field of AI that focuses on human-computer interaction using natural language."

# 1Ô∏è‚É£ Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
text = text.lower()

# 2Ô∏è‚É£ Lo·∫°i b·ªè d·∫•u c√¢u
text = text.translate(str.maketrans("", "", string.punctuation))

# 3Ô∏è‚É£ Tokenization
tokens = word_tokenize(text)

# 4Ô∏è‚É£ Lo·∫°i b·ªè t·ª´ d·ª´ng
stop_words = set(stopwords.words("english"))
filtered_tokens = [word for word in tokens if word not in stop_words]

# 5Ô∏è‚É£ Stemming (r√∫t g·ªçn t·ª´ g·ªëc)
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

# 6Ô∏è‚É£ Lemmatization (chuy·ªÉn v·ªÅ d·∫°ng nguy√™n th·ªÉ)
lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

# üî• Hi·ªÉn th·ªã k·∫øt qu·∫£
print("Original Text:", text)
print("\nTokenized Words:", tokens)
print("\nAfter Removing Stopwords:", filtered_tokens)
print("\nAfter Stemming:", stemmed_tokens)
print("\nAfter Lemmatization:", lemmatized_tokens)
